{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data Scraping\n",
    "\n",
    "This notebook aims to scrape data from seasons 1976 up to 2020. We will use beautifulsoup to scrape data on player statistics, team standings, and mvp award data from basketball-reference.\n",
    "\n",
    "The following resources were used for this part of the project:\n",
    "- https://medium.com/analytics-vidhya/intro-to-scraping-basketball-reference-data-8adcaa79664a\n",
    "- https://towardsdatascience.com/web-scraping-nba-stats-4b4f8c525994"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YEAR will be the range of years we will collect data from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = []\n",
    "\n",
    "for year in range(1976,2020):\n",
    "    YEAR.append(year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BasketballReferenceScraper class contains methods that will allow the collection of different types of data. scrapeStats will require a year and page_string as an input while scrapeStandings and scrapeMVP will only require the year. \n",
    "\n",
    "The scraped data will be saved as an csv file in the corresponding folder located at ../../input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasketballReferenceScraper():\n",
    "\n",
    "    def scrapeStats(self, YEAR, page_string):\n",
    "\n",
    "        # specify directory we want the csv files to go into\n",
    "        os.chdir('../../input/{}'.format(page_string))\n",
    "\n",
    "        for i in tqdm(range(len(YEAR))):\n",
    "\n",
    "            # year to scrape\n",
    "            year = YEAR[i]\n",
    "\n",
    "            # url of page to scrape data from\n",
    "            url = \"https://www.basketball-reference.com/leagues/NBA_{}_{}.html\".format(year, page_string)\n",
    "\n",
    "            # html from given url\n",
    "            html = urlopen(url)\n",
    "\n",
    "            # initialize BeautifulSoup class object\n",
    "            soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "\n",
    "            # extract text to get a list of header labels (exclude ranking order from basketball reference)\n",
    "            headers = [th.getText() for th in soup.findAll('tr')[0].findAll('th')[1:]]\n",
    "\n",
    "            # extract text to get a list of row values (exclude first header row)\n",
    "            rows = soup.findAll('tr')[1:]\n",
    "            player_stats = [[td.getText() for td in rows[i].findAll('td')] for i in range(len(rows))]\n",
    "\n",
    "            # create pandas dataframe to organize data extracted\n",
    "            stats = pd.DataFrame(player_stats, columns=headers)\n",
    "            \n",
    "            # gets the path which was changed to the specified folder\n",
    "            export_path = os.getcwd()\n",
    "\n",
    "            # saves to csv file labeled by the statistics type and year\n",
    "            stats.to_csv(export_path + '\\\\{}-{}.csv'.format(year, page_string))\n",
    "\n",
    "    def scrapeStandings(self, YEAR):\n",
    "\n",
    "        # specify directory we want the csv files to go into\n",
    "        os.chdir('../../input/standings')\n",
    "\n",
    "        for i in tqdm(range(len(YEAR))):\n",
    "\n",
    "            # year to scrape\n",
    "            year = YEAR[i]\n",
    "\n",
    "            # url of page to scrape data from\n",
    "            url = \"https://www.basketball-reference.com/leagues/NBA_{}_standings.html\".format(year)\n",
    "\n",
    "            # html from given url\n",
    "            html = urlopen(url)\n",
    "\n",
    "            # initialize BeautifulSoup class object\n",
    "            soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "\n",
    "            # extract text to get a list of headers\n",
    "            titles = [th.getText() for th in soup.findAll('tr')[0].findAll('th')[1:]]\n",
    "\n",
    "            # extract text for only column headers\n",
    "            headers = titles[0:titles.index(\"SRS\")+1]\n",
    "\n",
    "            # remove column headers from titles\n",
    "            titles = titles[titles.index(\"SRS\")+1:]\n",
    "\n",
    "            # get team names\n",
    "            try:\n",
    "                team_names = titles[0:titles.index(\"Easter Conference\")]\n",
    "            except:\n",
    "                team_names = titles\n",
    "\n",
    "            # remove non team names from list\n",
    "            for i in headers:\n",
    "                team_names.remove(i)\n",
    "\n",
    "            #remove W and western conference from team names\n",
    "            team_names.remove(\"Western Conference\")\n",
    "            try:\n",
    "                team_names.remove(\"W\")\n",
    "            except:\n",
    "                None\n",
    "\n",
    "            # list of divisions to remove from team names\n",
    "            divisions = [\"Atlantic Division\", \"Central Division\", \"Southeast Division\", \"Northwest Division\",\n",
    "                         \"Pacific Division\", \"Southwest Division\", \"Midwest Division\"]\n",
    "\n",
    "            # remove division from list\n",
    "            for division in divisions:\n",
    "                try:\n",
    "                    team_names.remove(division)\n",
    "                except:\n",
    "                    None\n",
    "\n",
    "            # then grab all data from rows except first row\n",
    "            rows = soup.findAll('tr')[1:]\n",
    "\n",
    "            # get the team standings\n",
    "            team_standings = [[td.getText() for td in rows[i].findAll('td')] for i in range(len(rows))]\n",
    "\n",
    "            # remove empty elements\n",
    "            team_standings = [e for e in team_standings if e != []]\n",
    "            # only keep needed rows\n",
    "            team_standings = team_standings[0:len(team_names)]\n",
    "\n",
    "            # add corresponding team name to the respective team standings\n",
    "            for i in range(0, len(team_standings)):\n",
    "                team_standings[i].insert(0, team_names[i])\n",
    "\n",
    "            # add team to headers\n",
    "            headers.insert(0, \"Team\")\n",
    "\n",
    "            # create pandas dataframe\n",
    "            year_standings = pd.DataFrame(team_standings, columns=headers)\n",
    "\n",
    "            # delete 'games behind' column\n",
    "            del year_standings['GB']\n",
    "\n",
    "            # add a column to dataframe to indicate playoff appearance\n",
    "            year_standings[\"Playoffs\"] = [\"Y\" if \"*\" in ele else \"N\" for ele in year_standings[\"Team\"]]\n",
    "            # remove * from team names\n",
    "            year_standings[\"Team\"] = [ele.replace('*', '') for ele in year_standings[\"Team\"]]\n",
    "            # add losing season indicator (win % < .5)\n",
    "            year_standings[\"Losing_season\"] = [\"Y\" if float(ele) < .5 else \"N\" for ele in year_standings[\"W/L%\"]]\n",
    "\n",
    "            # gets the path which was changed to the specified folder\n",
    "            export_path = os.getcwd()\n",
    "\n",
    "            # saves to csv file labeled by the statistics type and year\n",
    "            year_standings.to_csv(export_path + '\\\\{}-standings.csv'.format(year))\n",
    "\n",
    "    def scrapeMvp(self, YEAR):\n",
    "\n",
    "        # specify directory we want the csv files to go into\n",
    "        os.chdir('../../input/mvp_data')\n",
    "\n",
    "        for i in tqdm(range(len(YEAR))):\n",
    "\n",
    "            # year to scrape\n",
    "            year = YEAR[i]\n",
    "\n",
    "            # url of page to scrape data from\n",
    "            url = \"https://www.basketball-reference.com/awards/awards_{}.html\".format(year)\n",
    "\n",
    "            # html from given url\n",
    "            html = urlopen(url)\n",
    "\n",
    "            # initialize BeautifulSoup class object\n",
    "            soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "\n",
    "            # extract text to get a list of header labels\n",
    "            headers = [th.getText() for th in soup.findAll('tr')[1].findAll('th')[1:]]\n",
    "\n",
    "            # extract text to get a list of row values\n",
    "            rows = soup.findAll('tr')[2:]\n",
    "            player_awards = [[td.getText() for td in rows[i].findAll('td')] for i in range(len(rows))]\n",
    "\n",
    "            # remove non-mvp data\n",
    "            for i in range(len(player_awards)):\n",
    "                if player_awards[i] == []:\n",
    "                    player_awards = player_awards[0:i]\n",
    "                    break\n",
    "\n",
    "            # create pandas dataframe to organize data extracted\n",
    "            awards = pd.DataFrame(player_awards, columns=headers)\n",
    "\n",
    "            # gets the path which was changed to the specified folder\n",
    "            export_path = os.getcwd()\n",
    "\n",
    "            # saves to csv file labeled by the statistics type and year\n",
    "            awards.to_csv(export_path + '\\\\{}-awards.csv'.format(year))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape = BasketballReferenceScraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 44/44 [00:11<00:00,  4.00it/s]\n"
     ]
    }
   ],
   "source": [
    "scrape.scrapeMvp(YEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 44/44 [00:10<00:00,  4.20it/s]\n"
     ]
    }
   ],
   "source": [
    "scrape.scrapeStandings(YEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 44/44 [00:59<00:00,  1.35s/it]\n"
     ]
    }
   ],
   "source": [
    "scrape.scrapeStats(YEAR, 'advanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████████████████████████████████████████████████████████████████▉             | 37/44 [00:45<00:11,  1.61s/it]"
     ]
    }
   ],
   "source": [
    "scrape.scrapeStats(YEAR, 'per_game')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
